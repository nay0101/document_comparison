# Document Comparison (Language) — Documentation

## What this app does

The Document Comparison (Language) app compares two PDFs that are intended to be equivalent in different languages (e.g., English vs Bahasa Malaysia). It:

- Extracts text (and tables) from both PDFs
- Runs an LLM-driven, word-level discrepancy scan
- Produces a structured list of discrepancy “flags”
- Generates suggested rewrites for each flagged excerpt (per document)
- Lets you download a PDF report of the findings

The Streamlit UI entrypoint is `app.py`.

## Workflow (end-to-end)

1. **Upload PDFs** (`app.py`)
   - **Input:** Streamlit uploads (`session_state.doc1`, `session_state.doc2`) as `UploadedFile` objects (PDF).
   - **Output:** Stored in Streamlit session state; no processing yet.
2. **Prepare bytes + invoke chain** (click **Compare** in `app.py`)
   - **Input:** `UploadedFile.getvalue()` → raw `bytes` for each PDF.
   - **Transform:** `bytes` → `io.BytesIO(bytes)` for each file.
   - **Call:** `DocumentComparisonGPT.invoke_chain(doc1_file_bytes=BytesIO(...), doc2_file_bytes=BytesIO(...), chat_id=...)`.
3. **Extract text + tables** (`DocumentComparisonGPT.invoke_chain`)
   - **Input:** `BytesIO` PDFs.
   - **Transform:** `DocumentProcessor.extract_with_pdfplumber(...)` per PDF:
     - `pdfplumber` page text → concatenated plain text
     - detected tables → Markdown tables (via `tabulate`)
     - pages separated by `---`
   - **Output:** `doc1` and `doc2` as large strings (Markdown-ish).
4. **Build comparison prompt + run LLM** (`DocumentComparisonGPT._comparison_chain(...).stream(...)`)
   - **Input:** `{"doc1": <string>, "doc2": <string>}`.
   - **Transform:** LangChain prompt injects both strings into the system’s “linguistic analyst” instruction.
   - **Output:** Streamed text chunks that should contain a JSON object with `total` + `flags`.
5. **Parse model output → structured flags** (`DocumentComparisonGPT.invoke_chain`)
   - **Input:** Accumulated output string.
   - **Transform:**
     - If a ```json fenced block exists, extract its content.
     - `json.loads(...)` → Python dict.
   - **Output:** `flag_list = result_dict["flags"]` (list of discrepancy objects).
6. **Generate rewrite suggestions (per flag)** (`SuggestionChain.invoke_suggestion_chain`)
   - **Input:** For each flag:
     - `flag["doc1"]["content"]`, `flag["doc2"]["content"]`, and `flag["explanation"]`.
   - **Transform:** LLM returns JSON (often fenced) → `DocumentProcessor.remove_code_fences(...)` → `json.loads(...)`.
   - **Output:** `flag["suggestions"] = {"document1_suggestions": [...], "document2_suggestions": [...]}`.
7. **Render + export** (`app.py` + `helpers/reports.py`)
   - **Input:** Final `{"flags": [...]}` stored in `session_state.result`.
   - **Transform (export):** `MarkdownPDFConverter.generate_report(...)` builds Markdown text → converts to PDF bytes.
   - **Output:** Downloadable `report.pdf`.

## Quickstart

1. Install dependencies:

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

2. Configure environment:

```bash
cp .env.example .env
```

Set at least `OPENAI_API_KEY` (or other supported provider keys; see “Configuration”).

3. Run the app:

```bash
streamlit run app.py --server.port 8501
```

## How to use (UI)

1. Upload “First Document” (PDF) and “Second Document” (PDF)
2. Click **Compare**
3. Review:
   - Discrepancy location
   - Side-by-side excerpts (`doc1.content` vs `doc2.content`)
   - `discrepancies` list + `explanation`
   - Suggested rewrites (“After”) for each document
4. Click **Download Report** to export a PDF

## Output shape

The comparison result stored in session state is shaped like:

```json
{
  "flags": [
    {
      "location": "Section/paragraph identifier",
      "doc1": { "content": "..." },
      "doc2": { "content": "..." },
      "discrepancies": ["..."],
      "explanation": "...",
      "suggestions": {
        "document1_suggestions": [{ "version": "v1", "modification": "..." }],
        "document2_suggestions": [{ "version": "v1", "modification": "..." }]
      }
    }
  ]
}
```

Notes:

- In the UI, only `suggestion["modification"]` is displayed.
- The PDF report is generated by `helpers/reports.py:MarkdownPDFConverter.generate_report`.

## How it works (code map)

- `app.py`: Streamlit UI; file upload; renders flags; report download
- `helpers/document_comparison_gpt.py:DocumentComparisonGPT`
  - Extracts both documents via `helpers/document_processor.py:DocumentProcessor.extract_with_pdfplumber`
  - Executes the comparison prompt (streamed) and parses JSON from the model output
  - Enriches each flag with rewrite suggestions via `helpers/suggestions.py:SuggestionChain`
- `helpers/document_processor.py`
  - `extract_with_pdfplumber`: extracts per-page text and tables into Markdown-ish text
  - Also contains alternative extractors (e.g., `extract_with_docling` via Runpod)

## Function reference

### `app.py` (Streamlit UI)

- Uploads: two `st.file_uploader(...)` widgets populate `session_state.doc1` / `session_state.doc2`.
- Compare action: `st.button(..., key="compare_btn")` triggers a new `DocumentComparisonGPT()` and calls `invoke_chain(...)`.
- Results rendering: iterates `final_result_json["flags"]` and displays:
  - `flag["location"]`
  - `flag["doc1"]["content"]` vs `flag["doc2"]["content"]`
  - `flag["discrepancies"]`
  - `flag["suggestions"]` (if present)
- Export: `MarkdownPDFConverter.generate_report(...)` and `st.download_button(...)`.

### `helpers/document_comparison_gpt.py:DocumentComparisonGPT`

- `__init__(chat_model_name="gpt-4.1", temperature=0.1)`

  - Creates the chat model via `helpers/llm_integrations.py:get_llm`.
  - Creates a `DocumentProcessor` and a `SuggestionChain`.
  - Initializes internal cache fields `self.doc1` / `self.doc2` (so repeat calls can reuse extracted text).

- `invoke_chain(doc1_file_bytes, doc2_file_bytes, prev_flag="", remaining_flags=0, chat_id=None) -> dict`

  - If `self.doc1`/`self.doc2` are empty, extracts both PDFs via `DocumentProcessor.extract_with_pdfplumber(...)`.
  - Runs the comparison prompt as a streaming chain via `_comparison_chain(...).stream(...)`.
  - Accumulates the streamed output string into `result`.
  - Attempts to extract a JSON object:
    - If it finds a ```json fenced block, it takes the inner object.
    - It then calls `json.loads(...)` on the extracted string.
  - Reads `flags` from the parsed object and appends them to `self.flag_list`.
  - For each flag in `self.flag_list`, calls `SuggestionChain.invoke_suggestion_chain(...)` and stores the returned JSON in `flag["suggestions"]`.
  - Returns `{"flags": ...}`.

- `_comparison_chain(prev_flag, remaining_flags) -> Runnable`
  - Builds the system instruction for a “meticulous linguistic analyst”.
  - Produces a runnable LangChain pipeline that expects `{"doc1": "...", "doc2": "..."}`.
  - The system instruction asks the model to output JSON with `total` and `flags`.

### `helpers/suggestions.py:SuggestionChain`

- `_suggestion_chain() -> Runnable`

  - Creates a prompt that takes `document1`, `document2`, `explanation`, `target_documents`, and `k`.
  - Returns a chain that produces a JSON string.

- `invoke_suggestion_chain(document1, document2, explanation, target_documents, k=1) -> dict`
  - Runs `_suggestion_chain().invoke(...)`.
  - Parses JSON after removing markdown fences via `DocumentProcessor.remove_code_fences(...)`.
  - Returns a dict like `{"document1_suggestions": [...], "document2_suggestions": [...]}`.

### `helpers/document_processor.py:DocumentProcessor` (used by the comparison)

- `extract_with_pdfplumber(pdf_bytes) -> str`

  - Iterates pages with `pdfplumber`.
  - Extracts plain text (`page.extract_text()`).
  - Extracts tables (`page.extract_tables()`), formats them as Markdown tables via `tabulate`.
  - Joins pages with `\n\n---\n\n` separators and returns the final string.

- `remove_code_fences(text) -> str`

  - If the text contains a ```json fenced block, calls `extract_json_from_markdown(...)`.
  - Otherwise strips leading/trailing triple-backticks for simple fenced outputs.

- `extract_json_from_markdown(text) -> str`

  - Locates the ```json block and extracts the raw content.
  - Calls `extract_json_object(...)` so nested braces don’t break extraction.

- `extract_json_object(text) -> str`
  - Finds the first `{` and returns the smallest substring with balanced `{...}` braces.

## Configuration

Environment variables are defined in `.env.example`:

- `OPENAI_API_KEY`: required for OpenAI models (used by `helpers/llm_integrations.py`)
- `GOOGLE_API_KEY`: required if you select a Gemini model
- `ANTHROPIC_API_KEY`: required if you use Claude models
- `LANGFUSE_*`: optional (Langfuse tracing via callbacks)
- `ENVIRONMENT`: optional (used to toggle some dev-only behavior; currently mostly commented out in `app.py`)
- `RUNPOD_API_KEY`: optional (only needed if you use `DocumentProcessor.extract_with_docling`)
